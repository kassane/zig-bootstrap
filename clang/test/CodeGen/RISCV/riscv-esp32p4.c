// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv32 -target-feature +xesppie -S -emit-llvm -O0 -o - %s \
// RUN: | FileCheck %s

#include <stdint.h>

// CHECK-LABEL: @test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DATA:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 10, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.h(i32 4, i32 2)
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.h.ld.ip(i32 4, i32 0, i32 [[TMP0]], i32 -96, i32 3)
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.h.ld.xp(i32 [[TMP1]], i32 5, i32 5, i32 [[TMP2]], i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.l(i32 6, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.l.ld.ip(i32 2, i32 3, i32 [[TMP3]], i32 -48, i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s16.qacc.l.ld.xp(i32 [[TMP4]], i32 7, i32 2, i32 [[TMP5]], i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.h(i32 4, i32 4)
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.h.ld.ip(i32 7, i32 4, i32 [[TMP6]], i32 -128, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.h.ld.xp(i32 [[TMP7]], i32 2, i32 3, i32 [[TMP8]], i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.l(i32 6, i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.l.ld.ip(i32 5, i32 5, i32 [[TMP9]], i32 16, i32 7)
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmulas.s8.qacc.l.ld.xp(i32 [[TMP10]], i32 4, i32 4, i32 [[TMP11]], i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc(i32 7, i32 6)
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc.ld.ip(i32 0, i32 4, i32 [[TMP12]], i32 96, i32 4)
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc.ld.xp(i32 [[TMP13]], i32 4, i32 4, i32 [[TMP14]], i32 7)
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc.st.ip(i32 2, i32 1, i32 7, i32 [[TMP15]], i32 -128)
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc.st.xp(i32 [[TMP16]], i32 1, i32 2, i32 6, i32 [[TMP17]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.xacc(i32 1, i32 3)
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.xacc.ld.ip(i32 7, i32 3, i32 [[TMP18]], i32 -96, i32 5)
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.xacc.ld.xp(i32 [[TMP19]], i32 3, i32 1, i32 [[TMP20]], i32 1)
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.xacc.st.ip(i32 2, i32 0, i32 0, i32 [[TMP21]], i32 64)
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.xacc.st.xp(i32 [[TMP22]], i32 6, i32 3, i32 6, i32 [[TMP23]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc(i32 0, i32 0)
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc.ld.ip(i32 0, i32 3, i32 [[TMP24]], i32 0, i32 7)
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc.ld.xp(i32 [[TMP25]], i32 4, i32 3, i32 [[TMP26]], i32 4)
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc.st.ip(i32 3, i32 3, i32 5, i32 [[TMP27]], i32 -64)
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc.st.xp(i32 [[TMP28]], i32 4, i32 7, i32 0, i32 [[TMP29]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.xacc(i32 3, i32 3)
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.xacc.ld.ip(i32 3, i32 2, i32 [[TMP30]], i32 0, i32 5)
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.xacc.ld.xp(i32 [[TMP31]], i32 6, i32 3, i32 [[TMP32]], i32 0)
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.xacc.st.ip(i32 1, i32 7, i32 7, i32 [[TMP33]], i32 -32)
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.xacc.st.xp(i32 [[TMP34]], i32 6, i32 7, i32 6, i32 [[TMP35]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc(i32 5, i32 4)
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc.ld.ip(i32 5, i32 2, i32 [[TMP36]], i32 64, i32 6)
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc.ld.xp(i32 [[TMP37]], i32 5, i32 7, i32 [[TMP38]], i32 7)
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc.st.ip(i32 1, i32 4, i32 3, i32 [[TMP39]], i32 -96)
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc.st.xp(i32 [[TMP40]], i32 5, i32 0, i32 2, i32 [[TMP41]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.xacc(i32 0, i32 7)
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.xacc.ld.ip(i32 6, i32 6, i32 [[TMP42]], i32 -96, i32 4)
// CHECK-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP44:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.xacc.ld.xp(i32 [[TMP43]], i32 6, i32 5, i32 [[TMP44]], i32 6)
// CHECK-NEXT:    [[TMP45:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.xacc.st.ip(i32 3, i32 0, i32 4, i32 [[TMP45]], i32 64)
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.xacc.st.xp(i32 [[TMP46]], i32 1, i32 0, i32 4, i32 [[TMP47]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc(i32 5, i32 4)
// CHECK-NEXT:    [[TMP48:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc.ld.ip(i32 5, i32 3, i32 [[TMP48]], i32 80, i32 5)
// CHECK-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP50:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc.ld.xp(i32 [[TMP49]], i32 4, i32 7, i32 [[TMP50]], i32 4)
// CHECK-NEXT:    [[TMP51:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc.st.ip(i32 3, i32 3, i32 5, i32 [[TMP51]], i32 -96)
// CHECK-NEXT:    [[TMP52:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP53:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc.st.xp(i32 [[TMP52]], i32 6, i32 7, i32 3, i32 [[TMP53]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.xacc(i32 0, i32 1)
// CHECK-NEXT:    [[TMP54:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.xacc.ld.ip(i32 6, i32 0, i32 [[TMP54]], i32 -32, i32 7)
// CHECK-NEXT:    [[TMP55:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP56:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.xacc.ld.xp(i32 [[TMP55]], i32 3, i32 3, i32 [[TMP56]], i32 5)
// CHECK-NEXT:    [[TMP57:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.xacc.st.ip(i32 7, i32 0, i32 4, i32 [[TMP57]], i32 32)
// CHECK-NEXT:    [[TMP58:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP59:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.xacc.st.xp(i32 [[TMP58]], i32 1, i32 0, i32 0, i32 [[TMP59]])
// CHECK-NEXT:    [[TMP60:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s16.qacc.ldbc.incp(i32 3, i32 6, i32 [[TMP60]], i32 7)
// CHECK-NEXT:    [[TMP61:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.s8.qacc.ldbc.incp(i32 5, i32 3, i32 [[TMP61]], i32 6)
// CHECK-NEXT:    [[TMP62:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u16.qacc.ldbc.incp(i32 0, i32 3, i32 [[TMP62]], i32 2)
// CHECK-NEXT:    [[TMP63:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmulas.u8.qacc.ldbc.incp(i32 4, i32 7, i32 [[TMP63]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.s16.qacc(i32 7, i32 7, i32 4)
// CHECK-NEXT:    [[TMP64:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.s16.qacc.ld.incp(i32 7, i32 7, i32 [[TMP64]], i32 4, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.s8.qacc(i32 7, i32 0, i32 7)
// CHECK-NEXT:    [[TMP65:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.s8.qacc.ld.incp(i32 5, i32 6, i32 [[TMP65]], i32 15, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.u16.qacc(i32 7, i32 0, i32 10)
// CHECK-NEXT:    [[TMP66:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.u16.qacc.ld.incp(i32 7, i32 6, i32 [[TMP66]], i32 1, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.u8.qacc(i32 3, i32 6, i32 5)
// CHECK-NEXT:    [[TMP67:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsmulas.u8.qacc.ld.incp(i32 6, i32 1, i32 [[TMP67]], i32 4, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s16(i32 2, i32 1, i32 3, i32 1)
// CHECK-NEXT:    [[TMP68:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s16.ld.incp(i32 2, i32 7, i32 [[TMP68]], i32 0, i32 5, i32 0)
// CHECK-NEXT:    [[TMP69:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s16.st.incp(i32 7, i32 4, i32 6, i32 [[TMP69]], i32 2, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s8(i32 5, i32 7, i32 2, i32 4)
// CHECK-NEXT:    [[TMP70:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s8.ld.incp(i32 0, i32 6, i32 [[TMP70]], i32 2, i32 7, i32 5)
// CHECK-NEXT:    [[TMP71:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.s8.st.incp(i32 1, i32 6, i32 5, i32 [[TMP71]], i32 0, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u16(i32 7, i32 4, i32 0, i32 0)
// CHECK-NEXT:    [[TMP72:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u16.ld.incp(i32 2, i32 0, i32 [[TMP72]], i32 3, i32 1, i32 1)
// CHECK-NEXT:    [[TMP73:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u16.st.incp(i32 4, i32 3, i32 4, i32 [[TMP73]], i32 1, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u8(i32 3, i32 4, i32 1, i32 5)
// CHECK-NEXT:    [[TMP74:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u8.ld.incp(i32 5, i32 0, i32 [[TMP74]], i32 1, i32 5, i32 1)
// CHECK-NEXT:    [[TMP75:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.cmul.u8.st.incp(i32 2, i32 7, i32 4, i32 [[TMP75]], i32 3, i32 1)
// CHECK-NEXT:    [[TMP76:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.s16.a(i32 2, i32 [[TMP76]])
// CHECK-NEXT:    [[TMP77:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.s32.a(i32 0, i32 [[TMP77]])
// CHECK-NEXT:    [[TMP78:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.s8.a(i32 7, i32 [[TMP78]])
// CHECK-NEXT:    [[TMP79:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.u16.a(i32 4, i32 [[TMP79]])
// CHECK-NEXT:    [[TMP80:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.u32.a(i32 4, i32 [[TMP80]])
// CHECK-NEXT:    [[TMP81:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.max.u8.a(i32 3, i32 [[TMP81]])
// CHECK-NEXT:    [[TMP82:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.s16.a(i32 0, i32 [[TMP82]])
// CHECK-NEXT:    [[TMP83:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.s32.a(i32 7, i32 [[TMP83]])
// CHECK-NEXT:    [[TMP84:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.s8.a(i32 4, i32 [[TMP84]])
// CHECK-NEXT:    [[TMP85:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.u16.a(i32 7, i32 [[TMP85]])
// CHECK-NEXT:    [[TMP86:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.u32.a(i32 6, i32 [[TMP86]])
// CHECK-NEXT:    [[TMP87:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.min.u8.a(i32 1, i32 [[TMP87]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vabs.16(i32 7, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vabs.32(i32 0, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vabs.8(i32 5, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s16(i32 0, i32 4, i32 0)
// CHECK-NEXT:    [[TMP88:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s16.ld.incp(i32 4, i32 2, i32 [[TMP88]], i32 0, i32 7)
// CHECK-NEXT:    [[TMP89:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s16.st.incp(i32 5, i32 7, i32 0, i32 [[TMP89]], i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s32(i32 6, i32 5, i32 0)
// CHECK-NEXT:    [[TMP90:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s32.ld.incp(i32 5, i32 6, i32 [[TMP90]], i32 0, i32 2)
// CHECK-NEXT:    [[TMP91:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s32.st.incp(i32 7, i32 7, i32 0, i32 [[TMP91]], i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s8(i32 6, i32 5, i32 5)
// CHECK-NEXT:    [[TMP92:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s8.ld.incp(i32 2, i32 4, i32 [[TMP92]], i32 6, i32 7)
// CHECK-NEXT:    [[TMP93:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.s8.st.incp(i32 4, i32 6, i32 4, i32 [[TMP93]], i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u16(i32 0, i32 6, i32 5)
// CHECK-NEXT:    [[TMP94:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u16.ld.incp(i32 6, i32 7, i32 [[TMP94]], i32 5, i32 1)
// CHECK-NEXT:    [[TMP95:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u16.st.incp(i32 1, i32 3, i32 4, i32 [[TMP95]], i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u32(i32 7, i32 3, i32 0)
// CHECK-NEXT:    [[TMP96:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u32.ld.incp(i32 0, i32 4, i32 [[TMP96]], i32 5, i32 5)
// CHECK-NEXT:    [[TMP97:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u32.st.incp(i32 1, i32 5, i32 6, i32 [[TMP97]], i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u8(i32 0, i32 1, i32 5)
// CHECK-NEXT:    [[TMP98:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u8.ld.incp(i32 5, i32 1, i32 [[TMP98]], i32 2, i32 6)
// CHECK-NEXT:    [[TMP99:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vadd.u8.st.incp(i32 1, i32 7, i32 4, i32 [[TMP99]], i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vclamp.s16(i32 3, i32 12, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s16(i32 1, i32 2, i32 2)
// CHECK-NEXT:    [[TMP100:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s16.ld.incp(i32 3, i32 0, i32 [[TMP100]], i32 5, i32 1)
// CHECK-NEXT:    [[TMP101:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s16.st.incp(i32 0, i32 4, i32 2, i32 [[TMP101]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s32(i32 0, i32 2, i32 4)
// CHECK-NEXT:    [[TMP102:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s32.ld.incp(i32 3, i32 5, i32 [[TMP102]], i32 3, i32 6)
// CHECK-NEXT:    [[TMP103:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s32.st.incp(i32 6, i32 0, i32 7, i32 [[TMP103]], i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s8(i32 1, i32 0, i32 3)
// CHECK-NEXT:    [[TMP104:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s8.ld.incp(i32 1, i32 6, i32 [[TMP104]], i32 6, i32 6)
// CHECK-NEXT:    [[TMP105:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.s8.st.incp(i32 2, i32 7, i32 1, i32 [[TMP105]], i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u16(i32 6, i32 6, i32 3)
// CHECK-NEXT:    [[TMP106:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u16.ld.incp(i32 5, i32 2, i32 [[TMP106]], i32 2, i32 1)
// CHECK-NEXT:    [[TMP107:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u16.st.incp(i32 3, i32 6, i32 2, i32 [[TMP107]], i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u32(i32 2, i32 3, i32 3)
// CHECK-NEXT:    [[TMP108:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u32.ld.incp(i32 1, i32 4, i32 [[TMP108]], i32 5, i32 5)
// CHECK-NEXT:    [[TMP109:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u32.st.incp(i32 4, i32 2, i32 1, i32 [[TMP109]], i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u8(i32 7, i32 0, i32 4)
// CHECK-NEXT:    [[TMP110:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u8.ld.incp(i32 1, i32 5, i32 [[TMP110]], i32 4, i32 7)
// CHECK-NEXT:    [[TMP111:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmax.u8.st.incp(i32 1, i32 2, i32 5, i32 [[TMP111]], i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s16(i32 5, i32 1, i32 7)
// CHECK-NEXT:    [[TMP112:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s16.ld.incp(i32 7, i32 6, i32 [[TMP112]], i32 6, i32 4)
// CHECK-NEXT:    [[TMP113:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s16.st.incp(i32 7, i32 0, i32 6, i32 [[TMP113]], i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s32(i32 7, i32 4, i32 7)
// CHECK-NEXT:    [[TMP114:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s32.ld.incp(i32 0, i32 1, i32 [[TMP114]], i32 5, i32 4)
// CHECK-NEXT:    [[TMP115:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s32.st.incp(i32 1, i32 6, i32 7, i32 [[TMP115]], i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s8(i32 5, i32 6, i32 4)
// CHECK-NEXT:    [[TMP116:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s8.ld.incp(i32 1, i32 6, i32 [[TMP116]], i32 6, i32 5)
// CHECK-NEXT:    [[TMP117:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.s8.st.incp(i32 7, i32 7, i32 6, i32 [[TMP117]], i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u16(i32 7, i32 1, i32 1)
// CHECK-NEXT:    [[TMP118:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u16.ld.incp(i32 6, i32 0, i32 [[TMP118]], i32 3, i32 0)
// CHECK-NEXT:    [[TMP119:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u16.st.incp(i32 0, i32 7, i32 5, i32 [[TMP119]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u32(i32 6, i32 5, i32 0)
// CHECK-NEXT:    [[TMP120:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u32.ld.incp(i32 3, i32 7, i32 [[TMP120]], i32 1, i32 4)
// CHECK-NEXT:    [[TMP121:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u32.st.incp(i32 1, i32 0, i32 2, i32 [[TMP121]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u8(i32 2, i32 0, i32 7)
// CHECK-NEXT:    [[TMP122:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u8.ld.incp(i32 4, i32 2, i32 [[TMP122]], i32 4, i32 3)
// CHECK-NEXT:    [[TMP123:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmin.u8.st.incp(i32 1, i32 7, i32 4, i32 [[TMP123]], i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s16(i32 7, i32 5, i32 3)
// CHECK-NEXT:    [[TMP124:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s16.ld.incp(i32 5, i32 4, i32 [[TMP124]], i32 1, i32 6)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s16.s8xs8(i32 7, i32 6, i32 4, i32 4)
// CHECK-NEXT:    [[TMP125:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s16.st.incp(i32 0, i32 1, i32 5, i32 [[TMP125]], i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s32.s16xs16(i32 5, i32 3, i32 1, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s8(i32 1, i32 6, i32 0)
// CHECK-NEXT:    [[TMP126:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s8.ld.incp(i32 2, i32 1, i32 [[TMP126]], i32 6, i32 5)
// CHECK-NEXT:    [[TMP127:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.s8.st.incp(i32 5, i32 2, i32 1, i32 [[TMP127]], i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u16(i32 7, i32 3, i32 6)
// CHECK-NEXT:    [[TMP128:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u16.ld.incp(i32 3, i32 3, i32 [[TMP128]], i32 2, i32 0)
// CHECK-NEXT:    [[TMP129:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u16.st.incp(i32 6, i32 5, i32 0, i32 [[TMP129]], i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u8(i32 2, i32 2, i32 7)
// CHECK-NEXT:    [[TMP130:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u8.ld.incp(i32 1, i32 1, i32 [[TMP130]], i32 6, i32 7)
// CHECK-NEXT:    [[TMP131:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vmul.u8.st.incp(i32 5, i32 0, i32 6, i32 [[TMP131]], i32 2)
// CHECK-NEXT:    [[TMP132:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vprelu.s16(i32 [[TMP132]], i32 0, i32 7, i32 3)
// CHECK-NEXT:    [[TMP133:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vprelu.s8(i32 [[TMP133]], i32 6, i32 6, i32 6)
// CHECK-NEXT:    [[TMP134:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP135:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vrelu.s16(i32 [[TMP134]], i32 [[TMP135]], i32 3)
// CHECK-NEXT:    [[TMP136:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP137:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vrelu.s8(i32 [[TMP136]], i32 [[TMP137]], i32 7)
// CHECK-NEXT:    [[TMP138:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsadds.s16(i32 [[TMP138]], i32 5, i32 4)
// CHECK-NEXT:    [[TMP139:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsadds.s8(i32 [[TMP139]], i32 6, i32 6)
// CHECK-NEXT:    [[TMP140:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsadds.u16(i32 [[TMP140]], i32 7, i32 2)
// CHECK-NEXT:    [[TMP141:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsadds.u8(i32 [[TMP141]], i32 2, i32 0)
// CHECK-NEXT:    [[TMP142:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP143:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.s16(i32 [[TMP142]], i32 [[TMP143]], i32 7, i32 5)
// CHECK-NEXT:    [[TMP144:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP145:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.s32(i32 [[TMP144]], i32 [[TMP145]], i32 2, i32 5)
// CHECK-NEXT:    [[TMP146:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP147:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.s8(i32 [[TMP146]], i32 [[TMP147]], i32 2, i32 5)
// CHECK-NEXT:    [[TMP148:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP149:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.u16(i32 [[TMP148]], i32 [[TMP149]], i32 0, i32 2)
// CHECK-NEXT:    [[TMP150:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP151:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.u32(i32 [[TMP150]], i32 [[TMP151]], i32 4, i32 2)
// CHECK-NEXT:    [[TMP152:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP153:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsat.u8(i32 [[TMP152]], i32 [[TMP153]], i32 0, i32 2)
// CHECK-NEXT:    [[TMP154:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vssubs.s16(i32 [[TMP154]], i32 3, i32 6)
// CHECK-NEXT:    [[TMP155:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vssubs.s8(i32 [[TMP155]], i32 5, i32 5)
// CHECK-NEXT:    [[TMP156:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vssubs.u16(i32 [[TMP156]], i32 6, i32 3)
// CHECK-NEXT:    [[TMP157:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vssubs.u8(i32 [[TMP157]], i32 0, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s16(i32 0, i32 5, i32 3)
// CHECK-NEXT:    [[TMP158:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s16.ld.incp(i32 0, i32 1, i32 [[TMP158]], i32 5, i32 3)
// CHECK-NEXT:    [[TMP159:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s16.st.incp(i32 5, i32 7, i32 7, i32 [[TMP159]], i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s32(i32 3, i32 0, i32 3)
// CHECK-NEXT:    [[TMP160:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s32.ld.incp(i32 1, i32 2, i32 [[TMP160]], i32 0, i32 2)
// CHECK-NEXT:    [[TMP161:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s32.st.incp(i32 4, i32 0, i32 0, i32 [[TMP161]], i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s8(i32 4, i32 1, i32 3)
// CHECK-NEXT:    [[TMP162:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s8.ld.incp(i32 3, i32 7, i32 [[TMP162]], i32 3, i32 5)
// CHECK-NEXT:    [[TMP163:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.s8.st.incp(i32 5, i32 7, i32 3, i32 [[TMP163]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u16(i32 4, i32 6, i32 5)
// CHECK-NEXT:    [[TMP164:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u16.ld.incp(i32 4, i32 7, i32 [[TMP164]], i32 0, i32 5)
// CHECK-NEXT:    [[TMP165:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u16.st.incp(i32 2, i32 2, i32 7, i32 [[TMP165]], i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u32(i32 0, i32 1, i32 2)
// CHECK-NEXT:    [[TMP166:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u32.ld.incp(i32 5, i32 6, i32 [[TMP166]], i32 3, i32 5)
// CHECK-NEXT:    [[TMP167:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u32.st.incp(i32 0, i32 1, i32 4, i32 [[TMP167]], i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u8(i32 4, i32 2, i32 7)
// CHECK-NEXT:    [[TMP168:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u8.ld.incp(i32 2, i32 7, i32 [[TMP168]], i32 3, i32 4)
// CHECK-NEXT:    [[TMP169:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vsub.u8.st.incp(i32 6, i32 4, i32 7, i32 [[TMP169]], i32 7)
// CHECK-NEXT:    [[TMP170:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP171:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP172:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.addx2(i32 [[TMP170]], i32 [[TMP171]], i32 [[TMP172]])
// CHECK-NEXT:    [[TMP173:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP174:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP175:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.addx4(i32 [[TMP173]], i32 [[TMP174]], i32 [[TMP175]])
// CHECK-NEXT:    [[TMP176:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP177:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP178:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sat(i32 [[TMP176]], i32 [[TMP177]], i32 [[TMP178]])
// CHECK-NEXT:    [[TMP179:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP180:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP181:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.subx2(i32 [[TMP179]], i32 [[TMP180]], i32 [[TMP181]])
// CHECK-NEXT:    [[TMP182:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP183:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP184:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.subx4(i32 [[TMP182]], i32 [[TMP183]], i32 [[TMP184]])
// CHECK-NEXT:    call void @llvm.riscv.esp.andq(i32 0, i32 1, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.notq(i32 0, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.orq(i32 0, i32 6, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.xorq(i32 7, i32 4, i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.s16(i32 6, i32 6, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.s32(i32 6, i32 2, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.s8(i32 7, i32 6, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.u16(i32 0, i32 2, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.u32(i32 6, i32 4, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.eq.u8(i32 6, i32 4, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.s16(i32 5, i32 3, i32 6)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.s32(i32 2, i32 4, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.s8(i32 7, i32 7, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.u16(i32 2, i32 7, i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.u32(i32 6, i32 4, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.gt.u8(i32 0, i32 4, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.s16(i32 4, i32 6, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.s32(i32 2, i32 4, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.s8(i32 3, i32 0, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.u16(i32 2, i32 4, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.u32(i32 2, i32 0, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vcmp.lt.u8(i32 0, i32 2, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.mov.s16.qacc(i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.mov.s8.qacc(i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.mov.u16.qacc(i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.mov.u8.qacc(i32 5)
// CHECK-NEXT:    [[TMP185:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.16.a(i32 2, i32 8, i32 [[TMP185]])
// CHECK-NEXT:    [[TMP186:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.16.q(i32 [[TMP186]], i32 12, i32 1)
// CHECK-NEXT:    [[TMP187:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.32.a(i32 4, i32 2, i32 [[TMP187]])
// CHECK-NEXT:    [[TMP188:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.32.q(i32 [[TMP188]], i32 1, i32 0)
// CHECK-NEXT:    [[TMP189:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.8.a(i32 0, i32 13, i32 [[TMP189]])
// CHECK-NEXT:    [[TMP190:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movi.8.q(i32 [[TMP190]], i32 14, i32 3)
// CHECK-NEXT:    [[TMP191:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.cfg(i32 [[TMP191]])
// CHECK-NEXT:    [[TMP192:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.fft.bit.width(i32 [[TMP192]])
// CHECK-NEXT:    [[TMP193:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP194:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.perf(i32 [[TMP193]], i32 [[TMP194]])
// CHECK-NEXT:    [[TMP195:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.sar(i32 [[TMP195]])
// CHECK-NEXT:    [[TMP196:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.sar.bytes(i32 [[TMP196]])
// CHECK-NEXT:    [[TMP197:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.xacc.h(i32 [[TMP197]])
// CHECK-NEXT:    [[TMP198:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.r.xacc.l(i32 [[TMP198]])
// CHECK-NEXT:    [[TMP199:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.cfg(i32 [[TMP199]])
// CHECK-NEXT:    [[TMP200:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.fft.bit.width(i32 [[TMP200]])
// CHECK-NEXT:    [[TMP201:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.perf(i32 [[TMP201]])
// CHECK-NEXT:    [[TMP202:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.sar(i32 [[TMP202]])
// CHECK-NEXT:    [[TMP203:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.sar.bytes(i32 [[TMP203]])
// CHECK-NEXT:    [[TMP204:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.xacc.h(i32 [[TMP204]])
// CHECK-NEXT:    [[TMP205:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movx.w.xacc.l(i32 [[TMP205]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vext.s16(i32 0, i32 4, i32 6)
// CHECK-NEXT:    call void @llvm.riscv.esp.vext.s8(i32 0, i32 7, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vext.u16(i32 1, i32 0, i32 6)
// CHECK-NEXT:    call void @llvm.riscv.esp.vext.u8(i32 4, i32 1, i32 6)
// CHECK-NEXT:    call void @llvm.riscv.esp.vunzip.16(i32 3, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vunzip.32(i32 6, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vunzip.8(i32 3, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vunzipt.16(i32 1, i32 5, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vunzipt.8(i32 7, i32 5, i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vzip.16(i32 2, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vzip.32(i32 0, i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.vzip.8(i32 6, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vzipt.16(i32 6, i32 3, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vzipt.8(i32 7, i32 0, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.zero.q(i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.zero.qacc()
// CHECK-NEXT:    call void @llvm.riscv.esp.zero.xacc()
// CHECK-NEXT:    [[TMP206:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.ams.s16.ld.incp(i32 1, i32 1, i32 3, i32 [[TMP206]], i32 0, i32 6, i32 0, i32 3)
// CHECK-NEXT:    [[TMP207:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.ams.s16.ld.incp.uaup(i32 3, i32 0, i32 1, i32 [[TMP207]], i32 0, i32 3, i32 3, i32 1)
// CHECK-NEXT:    [[TMP208:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.ams.s16.ld.r32.decp(i32 2, i32 3, i32 7, i32 [[TMP208]], i32 0, i32 1, i32 1, i32 4)
// CHECK-NEXT:    [[TMP209:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP210:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.ams.s16.st.incp(i32 4, i32 4, i32 0, i32 5, i32 [[TMP209]], i32 [[TMP210]], i32 1, i32 1)
// CHECK-NEXT:    [[TMP211:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.bitrev(i32 [[TMP211]], i32 6)
// CHECK-NEXT:    [[TMP212:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP213:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.cmul.s16.ld.xp(i32 [[TMP212]], i32 7, i32 0, i32 [[TMP213]], i32 2, i32 1, i32 2)
// CHECK-NEXT:    [[TMP214:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP215:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.cmul.s16.st.xp(i32 [[TMP214]], i32 6, i32 0, i32 7, i32 [[TMP215]], i32 0, i32 1, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.r2bf.s16(i32 2, i32 5, i32 0, i32 7, i32 5)
// CHECK-NEXT:    [[TMP216:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.r2bf.s16.st.incp(i32 1, i32 7, i32 [[TMP216]], i32 1, i32 6)
// CHECK-NEXT:    [[TMP217:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.fft.vst.r32.decp(i32 2, i32 [[TMP217]], i32 1)
// CHECK-NEXT:    [[TMP218:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.128.usar.ip(i32 [[TMP218]], i32 -464, i32 7)
// CHECK-NEXT:    [[TMP219:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP220:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.128.usar.xp(i32 [[TMP219]], i32 [[TMP220]], i32 0)
// CHECK-NEXT:    [[TMP221:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.xacc.ip(i32 [[TMP221]], i32 -224)
// CHECK-NEXT:    [[TMP222:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.s16.128.ip(i32 [[TMP222]], i32 288)
// CHECK-NEXT:    [[TMP223:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP224:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.s16.128.xp(i32 [[TMP223]], i32 [[TMP224]])
// CHECK-NEXT:    [[TMP225:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.s8.128.ip(i32 [[TMP225]], i32 -1408)
// CHECK-NEXT:    [[TMP226:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP227:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.s8.128.xp(i32 [[TMP226]], i32 [[TMP227]])
// CHECK-NEXT:    [[TMP228:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.u16.128.ip(i32 [[TMP228]], i32 -1440)
// CHECK-NEXT:    [[TMP229:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP230:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.u16.128.xp(i32 [[TMP229]], i32 [[TMP230]])
// CHECK-NEXT:    [[TMP231:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.u8.128.ip(i32 [[TMP231]], i32 -816)
// CHECK-NEXT:    [[TMP232:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP233:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldqa.u8.128.xp(i32 [[TMP232]], i32 [[TMP233]])
// CHECK-NEXT:    [[TMP234:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.16.ip(i32 [[TMP234]], i32 380, i32 2)
// CHECK-NEXT:    [[TMP235:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP236:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.16.xp(i32 [[TMP235]], i32 [[TMP236]], i32 3)
// CHECK-NEXT:    [[TMP237:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.32.ip(i32 [[TMP237]], i32 -292, i32 7)
// CHECK-NEXT:    [[TMP238:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP239:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.32.xp(i32 [[TMP238]], i32 [[TMP239]], i32 1)
// CHECK-NEXT:    [[TMP240:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.8.ip(i32 [[TMP240]], i32 -416, i32 5)
// CHECK-NEXT:    [[TMP241:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP242:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldbc.8.xp(i32 [[TMP241]], i32 [[TMP242]], i32 7)
// CHECK-NEXT:    [[TMP243:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.s16.ip(i32 [[TMP243]], i32 -80, i32 0, i32 3)
// CHECK-NEXT:    [[TMP244:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP245:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.s16.xp(i32 [[TMP244]], i32 [[TMP245]], i32 2, i32 5)
// CHECK-NEXT:    [[TMP246:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.s8.ip(i32 [[TMP246]], i32 0, i32 2, i32 7)
// CHECK-NEXT:    [[TMP247:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP248:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.s8.xp(i32 [[TMP247]], i32 [[TMP248]], i32 7, i32 5)
// CHECK-NEXT:    [[TMP249:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.u16.ip(i32 [[TMP249]], i32 32, i32 0, i32 6)
// CHECK-NEXT:    [[TMP250:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP251:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.u16.xp(i32 [[TMP250]], i32 [[TMP251]], i32 7, i32 6)
// CHECK-NEXT:    [[TMP252:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.u8.ip(i32 [[TMP252]], i32 -16, i32 3, i32 1)
// CHECK-NEXT:    [[TMP253:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP254:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldext.u8.xp(i32 [[TMP253]], i32 [[TMP254]], i32 5, i32 4)
// CHECK-NEXT:    [[TMP255:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vldhbc.16.incp(i32 [[TMP255]], i32 2, i32 3)
// CHECK-NEXT:    [[TMP256:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.qacc.h.h.128.ip(i32 [[TMP256]], i32 -240)
// CHECK-NEXT:    [[TMP257:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.qacc.h.l.128.ip(i32 [[TMP257]], i32 -32)
// CHECK-NEXT:    [[TMP258:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.qacc.l.h.128.ip(i32 [[TMP258]], i32 -64)
// CHECK-NEXT:    [[TMP259:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.qacc.l.l.128.ip(i32 [[TMP259]], i32 -80)
// CHECK-NEXT:    [[TMP260:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ld.ua.state.ip(i32 [[TMP260]], i32 1504)
// CHECK-NEXT:    [[TMP261:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.ldxq.32(i32 [[TMP261]], i32 6, i32 1, i32 7, i32 1)
// CHECK-NEXT:    [[TMP262:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.qacc.h.h.128.ip(i32 [[TMP262]], i32 -480)
// CHECK-NEXT:    [[TMP263:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.qacc.h.l.128.ip(i32 [[TMP263]], i32 -1712)
// CHECK-NEXT:    [[TMP264:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.qacc.l.h.128.ip(i32 [[TMP264]], i32 960)
// CHECK-NEXT:    [[TMP265:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.qacc.l.l.128.ip(i32 [[TMP265]], i32 1920)
// CHECK-NEXT:    [[TMP266:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.ua.state.ip(i32 [[TMP266]], i32 -1360)
// CHECK-NEXT:    [[TMP267:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.stxq.32(i32 [[TMP267]], i32 6, i32 2, i32 3, i32 0)
// CHECK-NEXT:    [[TMP268:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.128.ip(i32 [[TMP268]], i32 -1136, i32 0)
// CHECK-NEXT:    [[TMP269:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP270:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.128.xp(i32 [[TMP269]], i32 [[TMP270]], i32 5)
// CHECK-NEXT:    [[TMP271:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.h.64.ip(i32 [[TMP271]], i32 1008, i32 4)
// CHECK-NEXT:    [[TMP272:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP273:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.h.64.xp(i32 [[TMP272]], i32 [[TMP273]], i32 2)
// CHECK-NEXT:    [[TMP274:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.l.64.ip(i32 [[TMP274]], i32 -304, i32 6)
// CHECK-NEXT:    [[TMP275:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP276:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vld.l.64.xp(i32 [[TMP275]], i32 [[TMP276]], i32 6)
// CHECK-NEXT:    [[TMP277:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.128.ip(i32 0, i32 [[TMP277]], i32 -1216)
// CHECK-NEXT:    [[TMP278:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP279:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.128.xp(i32 [[TMP278]], i32 6, i32 [[TMP279]])
// CHECK-NEXT:    [[TMP280:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.h.64.ip(i32 1, i32 [[TMP280]], i32 -456)
// CHECK-NEXT:    [[TMP281:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP282:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.h.64.xp(i32 [[TMP281]], i32 2, i32 [[TMP282]])
// CHECK-NEXT:    [[TMP283:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.l.64.ip(i32 6, i32 [[TMP283]], i32 664)
// CHECK-NEXT:    [[TMP284:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP285:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.vst.l.64.xp(i32 [[TMP284]], i32 4, i32 [[TMP285]])
// CHECK-NEXT:    call void @llvm.riscv.esp.slci.2q(i32 2, i32 0, i32 14)
// CHECK-NEXT:    [[TMP286:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP287:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.slcxxp.2q(i32 [[TMP286]], i32 [[TMP287]], i32 0, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.src.q(i32 7, i32 3, i32 2)
// CHECK-NEXT:    [[TMP288:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.src.q.ld.ip(i32 1, i32 [[TMP288]], i32 4, i32 1168, i32 4)
// CHECK-NEXT:    [[TMP289:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP290:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.src.q.ld.xp(i32 [[TMP289]], i32 0, i32 [[TMP290]], i32 1, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.src.q.qup(i32 3, i32 3, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.srci.2q(i32 7, i32 4, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.s16.q.qacc(i32 2, i32 1, i32 5)
// CHECK-NEXT:    [[TMP291:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.s16.qacc(i32 [[TMP291]], i32 0, i32 7)
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.s8.q.qacc(i32 7, i32 0, i32 3)
// CHECK-NEXT:    [[TMP292:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.s8.qacc(i32 [[TMP292]], i32 1, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.u16.q.qacc(i32 6, i32 1, i32 0)
// CHECK-NEXT:    [[TMP293:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.u16.qacc(i32 [[TMP293]], i32 0, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.u8.q.qacc(i32 6, i32 0, i32 7)
// CHECK-NEXT:    [[TMP294:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcmb.u8.qacc(i32 [[TMP294]], i32 1, i32 2)
// CHECK-NEXT:    [[TMP295:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcq.128.st.incp(i32 0, i32 5, i32 [[TMP295]])
// CHECK-NEXT:    [[TMP296:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP297:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srcxxp.2q(i32 [[TMP296]], i32 [[TMP297]], i32 7, i32 5)
// CHECK-NEXT:    [[TMP298:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP299:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srs.s.xacc(i32 [[TMP298]], i32 [[TMP299]])
// CHECK-NEXT:    [[TMP300:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP301:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.srs.u.xacc(i32 [[TMP300]], i32 [[TMP301]])
// CHECK-NEXT:    call void @llvm.riscv.esp.vsl.32(i32 0, i32 3)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsld.16(i32 6, i32 4, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsld.32(i32 2, i32 7, i32 5)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsld.8(i32 1, i32 0, i32 0)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsr.s32(i32 6, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsr.u32(i32 3, i32 2)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsrd.16(i32 6, i32 2, i32 1)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsrd.32(i32 7, i32 5, i32 4)
// CHECK-NEXT:    call void @llvm.riscv.esp.vsrd.8(i32 2, i32 1, i32 4)
// CHECK-NEXT:    [[TMP302:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.s.xacc.ip(i32 [[TMP302]], i32 912)
// CHECK-NEXT:    [[TMP303:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.st.u.xacc.ip(i32 [[TMP303]], i32 -112)
// CHECK-NEXT:    ret void
//
void test() {
  uint32_t data = 10;
  __builtin_riscv_esp_vcmulas_s16_qacc_h(4, 2);
__builtin_riscv_esp_vcmulas_s16_qacc_h_ld_ip(4, 0, data, -96, 3);
__builtin_riscv_esp_vcmulas_s16_qacc_h_ld_xp(data, 5, 5, data, 5);
__builtin_riscv_esp_vcmulas_s16_qacc_l(6, 1);
__builtin_riscv_esp_vcmulas_s16_qacc_l_ld_ip(2, 3, data, -48, 3);
__builtin_riscv_esp_vcmulas_s16_qacc_l_ld_xp(data, 7, 2, data, 1);
__builtin_riscv_esp_vcmulas_s8_qacc_h(4, 4);
__builtin_riscv_esp_vcmulas_s8_qacc_h_ld_ip(7, 4, data, -128, 4);
__builtin_riscv_esp_vcmulas_s8_qacc_h_ld_xp(data, 2, 3, data, 1);
__builtin_riscv_esp_vcmulas_s8_qacc_l(6, 4);
__builtin_riscv_esp_vcmulas_s8_qacc_l_ld_ip(5, 5, data, 16, 7);
__builtin_riscv_esp_vcmulas_s8_qacc_l_ld_xp(data, 4, 4, data, 2);
__builtin_riscv_esp_vmulas_s16_qacc(7, 6);
__builtin_riscv_esp_vmulas_s16_qacc_ld_ip(0, 4, data, 96, 4);
__builtin_riscv_esp_vmulas_s16_qacc_ld_xp(data, 4, 4, data, 7);
__builtin_riscv_esp_vmulas_s16_qacc_st_ip(2, 1, 7, data, -128);
__builtin_riscv_esp_vmulas_s16_qacc_st_xp(data, 1, 2, 6, data);
__builtin_riscv_esp_vmulas_s16_xacc(1, 3);
__builtin_riscv_esp_vmulas_s16_xacc_ld_ip(7, 3, data, -96, 5);
__builtin_riscv_esp_vmulas_s16_xacc_ld_xp(data, 3, 1, data, 1);
__builtin_riscv_esp_vmulas_s16_xacc_st_ip(2, 0, 0, data, 64);
__builtin_riscv_esp_vmulas_s16_xacc_st_xp(data, 6, 3, 6, data);
__builtin_riscv_esp_vmulas_s8_qacc(0, 0);
__builtin_riscv_esp_vmulas_s8_qacc_ld_ip(0, 3, data, 0, 7);
__builtin_riscv_esp_vmulas_s8_qacc_ld_xp(data, 4, 3, data, 4);
__builtin_riscv_esp_vmulas_s8_qacc_st_ip(3, 3, 5, data, -64);
__builtin_riscv_esp_vmulas_s8_qacc_st_xp(data, 4, 7, 0, data);
__builtin_riscv_esp_vmulas_s8_xacc(3, 3);
__builtin_riscv_esp_vmulas_s8_xacc_ld_ip(3, 2, data, 0, 5);
__builtin_riscv_esp_vmulas_s8_xacc_ld_xp(data, 6, 3, data, 0);
__builtin_riscv_esp_vmulas_s8_xacc_st_ip(1, 7, 7, data, -32);
__builtin_riscv_esp_vmulas_s8_xacc_st_xp(data, 6, 7, 6, data);
__builtin_riscv_esp_vmulas_u16_qacc(5, 4);
__builtin_riscv_esp_vmulas_u16_qacc_ld_ip(5, 2, data, 64, 6);
__builtin_riscv_esp_vmulas_u16_qacc_ld_xp(data, 5, 7, data, 7);
__builtin_riscv_esp_vmulas_u16_qacc_st_ip(1, 4, 3, data, -96);
__builtin_riscv_esp_vmulas_u16_qacc_st_xp(data, 5, 0, 2, data);
__builtin_riscv_esp_vmulas_u16_xacc(0, 7);
__builtin_riscv_esp_vmulas_u16_xacc_ld_ip(6, 6, data, -96, 4);
__builtin_riscv_esp_vmulas_u16_xacc_ld_xp(data, 6, 5, data, 6);
__builtin_riscv_esp_vmulas_u16_xacc_st_ip(3, 0, 4, data, 64);
__builtin_riscv_esp_vmulas_u16_xacc_st_xp(data, 1, 0, 4, data);
__builtin_riscv_esp_vmulas_u8_qacc(5, 4);
__builtin_riscv_esp_vmulas_u8_qacc_ld_ip(5, 3, data, 80, 5);
__builtin_riscv_esp_vmulas_u8_qacc_ld_xp(data, 4, 7, data, 4);
__builtin_riscv_esp_vmulas_u8_qacc_st_ip(3, 3, 5, data, -96);
__builtin_riscv_esp_vmulas_u8_qacc_st_xp(data, 6, 7, 3, data);
__builtin_riscv_esp_vmulas_u8_xacc(0, 1);
__builtin_riscv_esp_vmulas_u8_xacc_ld_ip(6, 0, data, -32, 7);
__builtin_riscv_esp_vmulas_u8_xacc_ld_xp(data, 3, 3, data, 5);
__builtin_riscv_esp_vmulas_u8_xacc_st_ip(7, 0, 4, data, 32);
__builtin_riscv_esp_vmulas_u8_xacc_st_xp(data, 1, 0, 0, data);
__builtin_riscv_esp_vmulas_s16_qacc_ldbc_incp(3, 6, data, 7);
__builtin_riscv_esp_vmulas_s8_qacc_ldbc_incp(5, 3, data, 6);
__builtin_riscv_esp_vmulas_u16_qacc_ldbc_incp(0, 3, data, 2);
__builtin_riscv_esp_vmulas_u8_qacc_ldbc_incp(4, 7, data, 3);
__builtin_riscv_esp_vsmulas_s16_qacc(7, 7, 4);
__builtin_riscv_esp_vsmulas_s16_qacc_ld_incp(7, 7, data, 4, 1);
__builtin_riscv_esp_vsmulas_s8_qacc(7, 0, 7);
__builtin_riscv_esp_vsmulas_s8_qacc_ld_incp(5, 6, data, 15, 2);
__builtin_riscv_esp_vsmulas_u16_qacc(7, 0, 10);
__builtin_riscv_esp_vsmulas_u16_qacc_ld_incp(7, 6, data, 1, 0);
__builtin_riscv_esp_vsmulas_u8_qacc(3, 6, 5);
__builtin_riscv_esp_vsmulas_u8_qacc_ld_incp(6, 1, data, 4, 0);
__builtin_riscv_esp_cmul_s16(2, 1, 3, 1);
__builtin_riscv_esp_cmul_s16_ld_incp(2, 7, data, 0, 5, 0);
__builtin_riscv_esp_cmul_s16_st_incp(7, 4, 6, data, 2, 5);
__builtin_riscv_esp_cmul_s8(5, 7, 2, 4);
__builtin_riscv_esp_cmul_s8_ld_incp(0, 6, data, 2, 7, 5);
__builtin_riscv_esp_cmul_s8_st_incp(1, 6, 5, data, 0, 2);
__builtin_riscv_esp_cmul_u16(7, 4, 0, 0);
__builtin_riscv_esp_cmul_u16_ld_incp(2, 0, data, 3, 1, 1);
__builtin_riscv_esp_cmul_u16_st_incp(4, 3, 4, data, 1, 2);
__builtin_riscv_esp_cmul_u8(3, 4, 1, 5);
__builtin_riscv_esp_cmul_u8_ld_incp(5, 0, data, 1, 5, 1);
__builtin_riscv_esp_cmul_u8_st_incp(2, 7, 4, data, 3, 1);
__builtin_riscv_esp_max_s16_a(2, data);
__builtin_riscv_esp_max_s32_a(0, data);
__builtin_riscv_esp_max_s8_a(7, data);
__builtin_riscv_esp_max_u16_a(4, data);
__builtin_riscv_esp_max_u32_a(4, data);
__builtin_riscv_esp_max_u8_a(3, data);
__builtin_riscv_esp_min_s16_a(0, data);
__builtin_riscv_esp_min_s32_a(7, data);
__builtin_riscv_esp_min_s8_a(4, data);
__builtin_riscv_esp_min_u16_a(7, data);
__builtin_riscv_esp_min_u32_a(6, data);
__builtin_riscv_esp_min_u8_a(1, data);
__builtin_riscv_esp_vabs_16(7, 0);
__builtin_riscv_esp_vabs_32(0, 3);
__builtin_riscv_esp_vabs_8(5, 2);
__builtin_riscv_esp_vadd_s16(0, 4, 0);
__builtin_riscv_esp_vadd_s16_ld_incp(4, 2, data, 0, 7);
__builtin_riscv_esp_vadd_s16_st_incp(5, 7, 0, data, 5);
__builtin_riscv_esp_vadd_s32(6, 5, 0);
__builtin_riscv_esp_vadd_s32_ld_incp(5, 6, data, 0, 2);
__builtin_riscv_esp_vadd_s32_st_incp(7, 7, 0, data, 1);
__builtin_riscv_esp_vadd_s8(6, 5, 5);
__builtin_riscv_esp_vadd_s8_ld_incp(2, 4, data, 6, 7);
__builtin_riscv_esp_vadd_s8_st_incp(4, 6, 4, data, 7);
__builtin_riscv_esp_vadd_u16(0, 6, 5);
__builtin_riscv_esp_vadd_u16_ld_incp(6, 7, data, 5, 1);
__builtin_riscv_esp_vadd_u16_st_incp(1, 3, 4, data, 5);
__builtin_riscv_esp_vadd_u32(7, 3, 0);
__builtin_riscv_esp_vadd_u32_ld_incp(0, 4, data, 5, 5);
__builtin_riscv_esp_vadd_u32_st_incp(1, 5, 6, data, 1);
__builtin_riscv_esp_vadd_u8(0, 1, 5);
__builtin_riscv_esp_vadd_u8_ld_incp(5, 1, data, 2, 6);
__builtin_riscv_esp_vadd_u8_st_incp(1, 7, 4, data, 2);
__builtin_riscv_esp_vclamp_s16(3, 12, 5);
__builtin_riscv_esp_vmax_s16(1, 2, 2);
__builtin_riscv_esp_vmax_s16_ld_incp(3, 0, data, 5, 1);
__builtin_riscv_esp_vmax_s16_st_incp(0, 4, 2, data, 3);
__builtin_riscv_esp_vmax_s32(0, 2, 4);
__builtin_riscv_esp_vmax_s32_ld_incp(3, 5, data, 3, 6);
__builtin_riscv_esp_vmax_s32_st_incp(6, 0, 7, data, 4);
__builtin_riscv_esp_vmax_s8(1, 0, 3);
__builtin_riscv_esp_vmax_s8_ld_incp(1, 6, data, 6, 6);
__builtin_riscv_esp_vmax_s8_st_incp(2, 7, 1, data, 0);
__builtin_riscv_esp_vmax_u16(6, 6, 3);
__builtin_riscv_esp_vmax_u16_ld_incp(5, 2, data, 2, 1);
__builtin_riscv_esp_vmax_u16_st_incp(3, 6, 2, data, 0);
__builtin_riscv_esp_vmax_u32(2, 3, 3);
__builtin_riscv_esp_vmax_u32_ld_incp(1, 4, data, 5, 5);
__builtin_riscv_esp_vmax_u32_st_incp(4, 2, 1, data, 4);
__builtin_riscv_esp_vmax_u8(7, 0, 4);
__builtin_riscv_esp_vmax_u8_ld_incp(1, 5, data, 4, 7);
__builtin_riscv_esp_vmax_u8_st_incp(1, 2, 5, data, 7);
__builtin_riscv_esp_vmin_s16(5, 1, 7);
__builtin_riscv_esp_vmin_s16_ld_incp(7, 6, data, 6, 4);
__builtin_riscv_esp_vmin_s16_st_incp(7, 0, 6, data, 2);
__builtin_riscv_esp_vmin_s32(7, 4, 7);
__builtin_riscv_esp_vmin_s32_ld_incp(0, 1, data, 5, 4);
__builtin_riscv_esp_vmin_s32_st_incp(1, 6, 7, data, 4);
__builtin_riscv_esp_vmin_s8(5, 6, 4);
__builtin_riscv_esp_vmin_s8_ld_incp(1, 6, data, 6, 5);
__builtin_riscv_esp_vmin_s8_st_incp(7, 7, 6, data, 7);
__builtin_riscv_esp_vmin_u16(7, 1, 1);
__builtin_riscv_esp_vmin_u16_ld_incp(6, 0, data, 3, 0);
__builtin_riscv_esp_vmin_u16_st_incp(0, 7, 5, data, 3);
__builtin_riscv_esp_vmin_u32(6, 5, 0);
__builtin_riscv_esp_vmin_u32_ld_incp(3, 7, data, 1, 4);
__builtin_riscv_esp_vmin_u32_st_incp(1, 0, 2, data, 3);
__builtin_riscv_esp_vmin_u8(2, 0, 7);
__builtin_riscv_esp_vmin_u8_ld_incp(4, 2, data, 4, 3);
__builtin_riscv_esp_vmin_u8_st_incp(1, 7, 4, data, 4);
__builtin_riscv_esp_vmul_s16(7, 5, 3);
__builtin_riscv_esp_vmul_s16_ld_incp(5, 4, data, 1, 6);
__builtin_riscv_esp_vmul_s16_s8xs8(7, 6, 4, 4);
__builtin_riscv_esp_vmul_s16_st_incp(0, 1, 5, data, 7);
__builtin_riscv_esp_vmul_s32_s16xs16(5, 3, 1, 2);
__builtin_riscv_esp_vmul_s8(1, 6, 0);
__builtin_riscv_esp_vmul_s8_ld_incp(2, 1, data, 6, 5);
__builtin_riscv_esp_vmul_s8_st_incp(5, 2, 1, data, 7);
__builtin_riscv_esp_vmul_u16(7, 3, 6);
__builtin_riscv_esp_vmul_u16_ld_incp(3, 3, data, 2, 0);
__builtin_riscv_esp_vmul_u16_st_incp(6, 5, 0, data, 1);
__builtin_riscv_esp_vmul_u8(2, 2, 7);
__builtin_riscv_esp_vmul_u8_ld_incp(1, 1, data, 6, 7);
__builtin_riscv_esp_vmul_u8_st_incp(5, 0, 6, data, 2);
__builtin_riscv_esp_vprelu_s16(data, 0, 7, 3);
__builtin_riscv_esp_vprelu_s8(data, 6, 6, 6);
__builtin_riscv_esp_vrelu_s16(data, data, 3);
__builtin_riscv_esp_vrelu_s8(data, data, 7);
__builtin_riscv_esp_vsadds_s16(data, 5, 4);
__builtin_riscv_esp_vsadds_s8(data, 6, 6);
__builtin_riscv_esp_vsadds_u16(data, 7, 2);
__builtin_riscv_esp_vsadds_u8(data, 2, 0);
__builtin_riscv_esp_vsat_s16(data, data, 7, 5);
__builtin_riscv_esp_vsat_s32(data, data, 2, 5);
__builtin_riscv_esp_vsat_s8(data, data, 2, 5);
__builtin_riscv_esp_vsat_u16(data, data, 0, 2);
__builtin_riscv_esp_vsat_u32(data, data, 4, 2);
__builtin_riscv_esp_vsat_u8(data, data, 0, 2);
__builtin_riscv_esp_vssubs_s16(data, 3, 6);
__builtin_riscv_esp_vssubs_s8(data, 5, 5);
__builtin_riscv_esp_vssubs_u16(data, 6, 3);
__builtin_riscv_esp_vssubs_u8(data, 0, 3);
__builtin_riscv_esp_vsub_s16(0, 5, 3);
__builtin_riscv_esp_vsub_s16_ld_incp(0, 1, data, 5, 3);
__builtin_riscv_esp_vsub_s16_st_incp(5, 7, 7, data, 4);
__builtin_riscv_esp_vsub_s32(3, 0, 3);
__builtin_riscv_esp_vsub_s32_ld_incp(1, 2, data, 0, 2);
__builtin_riscv_esp_vsub_s32_st_incp(4, 0, 0, data, 5);
__builtin_riscv_esp_vsub_s8(4, 1, 3);
__builtin_riscv_esp_vsub_s8_ld_incp(3, 7, data, 3, 5);
__builtin_riscv_esp_vsub_s8_st_incp(5, 7, 3, data, 3);
__builtin_riscv_esp_vsub_u16(4, 6, 5);
__builtin_riscv_esp_vsub_u16_ld_incp(4, 7, data, 0, 5);
__builtin_riscv_esp_vsub_u16_st_incp(2, 2, 7, data, 3);
__builtin_riscv_esp_vsub_u32(0, 1, 2);
__builtin_riscv_esp_vsub_u32_ld_incp(5, 6, data, 3, 5);
__builtin_riscv_esp_vsub_u32_st_incp(0, 1, 4, data, 2);
__builtin_riscv_esp_vsub_u8(4, 2, 7);
__builtin_riscv_esp_vsub_u8_ld_incp(2, 7, data, 3, 4);
__builtin_riscv_esp_vsub_u8_st_incp(6, 4, 7, data, 7);
__builtin_riscv_esp_addx2(data, data, data);
__builtin_riscv_esp_addx4(data, data, data);
__builtin_riscv_esp_sat(data, data, data);
__builtin_riscv_esp_subx2(data, data, data);
__builtin_riscv_esp_subx4(data, data, data);
__builtin_riscv_esp_andq(0, 1, 4);
__builtin_riscv_esp_notq(0, 1);
__builtin_riscv_esp_orq(0, 6, 3);
__builtin_riscv_esp_xorq(7, 4, 7);
__builtin_riscv_esp_vcmp_eq_s16(6, 6, 3);
__builtin_riscv_esp_vcmp_eq_s32(6, 2, 1);
__builtin_riscv_esp_vcmp_eq_s8(7, 6, 0);
__builtin_riscv_esp_vcmp_eq_u16(0, 2, 5);
__builtin_riscv_esp_vcmp_eq_u32(6, 4, 3);
__builtin_riscv_esp_vcmp_eq_u8(6, 4, 5);
__builtin_riscv_esp_vcmp_gt_s16(5, 3, 6);
__builtin_riscv_esp_vcmp_gt_s32(2, 4, 5);
__builtin_riscv_esp_vcmp_gt_s8(7, 7, 4);
__builtin_riscv_esp_vcmp_gt_u16(2, 7, 7);
__builtin_riscv_esp_vcmp_gt_u32(6, 4, 2);
__builtin_riscv_esp_vcmp_gt_u8(0, 4, 4);
__builtin_riscv_esp_vcmp_lt_s16(4, 6, 5);
__builtin_riscv_esp_vcmp_lt_s32(2, 4, 1);
__builtin_riscv_esp_vcmp_lt_s8(3, 0, 2);
__builtin_riscv_esp_vcmp_lt_u16(2, 4, 1);
__builtin_riscv_esp_vcmp_lt_u32(2, 0, 5);
__builtin_riscv_esp_vcmp_lt_u8(0, 2, 5);
__builtin_riscv_esp_mov_s16_qacc(4);
__builtin_riscv_esp_mov_s8_qacc(5);
__builtin_riscv_esp_mov_u16_qacc(5);
__builtin_riscv_esp_mov_u8_qacc(5);
__builtin_riscv_esp_movi_16_a(2, 8, data);
__builtin_riscv_esp_movi_16_q(data, 12, 1);
__builtin_riscv_esp_movi_32_a(4, 2, data);
__builtin_riscv_esp_movi_32_q(data, 1, 0);
__builtin_riscv_esp_movi_8_a(0, 13, data);
__builtin_riscv_esp_movi_8_q(data, 14, 3);
__builtin_riscv_esp_movx_r_cfg(data);
__builtin_riscv_esp_movx_r_fft_bit_width(data);
__builtin_riscv_esp_movx_r_perf(data, data);
__builtin_riscv_esp_movx_r_sar(data);
__builtin_riscv_esp_movx_r_sar_bytes(data);
__builtin_riscv_esp_movx_r_xacc_h(data);
__builtin_riscv_esp_movx_r_xacc_l(data);
__builtin_riscv_esp_movx_w_cfg(data);
__builtin_riscv_esp_movx_w_fft_bit_width(data);
__builtin_riscv_esp_movx_w_perf(data);
__builtin_riscv_esp_movx_w_sar(data);
__builtin_riscv_esp_movx_w_sar_bytes(data);
__builtin_riscv_esp_movx_w_xacc_h(data);
__builtin_riscv_esp_movx_w_xacc_l(data);
__builtin_riscv_esp_vext_s16(0, 4, 6);
__builtin_riscv_esp_vext_s8(0, 7, 1);
__builtin_riscv_esp_vext_u16(1, 0, 6);
__builtin_riscv_esp_vext_u8(4, 1, 6);
__builtin_riscv_esp_vunzip_16(3, 2);
__builtin_riscv_esp_vunzip_32(6, 1);
__builtin_riscv_esp_vunzip_8(3, 5);
__builtin_riscv_esp_vunzipt_16(1, 5, 4);
__builtin_riscv_esp_vunzipt_8(7, 5, 7);
__builtin_riscv_esp_vzip_16(2, 2);
__builtin_riscv_esp_vzip_32(0, 7);
__builtin_riscv_esp_vzip_8(6, 4);
__builtin_riscv_esp_vzipt_16(6, 3, 0);
__builtin_riscv_esp_vzipt_8(7, 0, 1);
__builtin_riscv_esp_zero_q(3);
__builtin_riscv_esp_zero_qacc();
__builtin_riscv_esp_zero_xacc();
__builtin_riscv_esp_fft_ams_s16_ld_incp(1, 1, 3, data, 0, 6, 0, 3);
__builtin_riscv_esp_fft_ams_s16_ld_incp_uaup(3, 0, 1, data, 0, 3, 3, 1);
__builtin_riscv_esp_fft_ams_s16_ld_r32_decp(2, 3, 7, data, 0, 1, 1, 4);
__builtin_riscv_esp_fft_ams_s16_st_incp(4, 4, 0, 5, data, data, 1, 1);
__builtin_riscv_esp_fft_bitrev(data, 6);
__builtin_riscv_esp_fft_cmul_s16_ld_xp(data, 7, 0, data, 2, 1, 2);
__builtin_riscv_esp_fft_cmul_s16_st_xp(data, 6, 0, 7, data, 0, 1, 0);
__builtin_riscv_esp_fft_r2bf_s16(2, 5, 0, 7, 5);
__builtin_riscv_esp_fft_r2bf_s16_st_incp(1, 7, data, 1, 6);
__builtin_riscv_esp_fft_vst_r32_decp(2, data, 1);
__builtin_riscv_esp_ld_128_usar_ip(data, -464, 7);
__builtin_riscv_esp_ld_128_usar_xp(data, data, 0);
__builtin_riscv_esp_ld_xacc_ip(data, -224);
__builtin_riscv_esp_ldqa_s16_128_ip(data, 288);
__builtin_riscv_esp_ldqa_s16_128_xp(data, data);
__builtin_riscv_esp_ldqa_s8_128_ip(data, -1408);
__builtin_riscv_esp_ldqa_s8_128_xp(data, data);
__builtin_riscv_esp_ldqa_u16_128_ip(data, -1440);
__builtin_riscv_esp_ldqa_u16_128_xp(data, data);
__builtin_riscv_esp_ldqa_u8_128_ip(data, -816);
__builtin_riscv_esp_ldqa_u8_128_xp(data, data);
__builtin_riscv_esp_vldbc_16_ip(data, 380, 2);
__builtin_riscv_esp_vldbc_16_xp(data, data, 3);
__builtin_riscv_esp_vldbc_32_ip(data, -292, 7);
__builtin_riscv_esp_vldbc_32_xp(data, data, 1);
__builtin_riscv_esp_vldbc_8_ip(data, -416, 5);
__builtin_riscv_esp_vldbc_8_xp(data, data, 7);
__builtin_riscv_esp_vldext_s16_ip(data, -80, 0, 3);
__builtin_riscv_esp_vldext_s16_xp(data, data, 2, 5);
__builtin_riscv_esp_vldext_s8_ip(data, 0, 2, 7);
__builtin_riscv_esp_vldext_s8_xp(data, data, 7, 5);
__builtin_riscv_esp_vldext_u16_ip(data, 32, 0, 6);
__builtin_riscv_esp_vldext_u16_xp(data, data, 7, 6);
__builtin_riscv_esp_vldext_u8_ip(data, -16, 3, 1);
__builtin_riscv_esp_vldext_u8_xp(data, data, 5, 4);
__builtin_riscv_esp_vldhbc_16_incp(data, 2, 3);
__builtin_riscv_esp_ld_qacc_h_h_128_ip(data, -240);
__builtin_riscv_esp_ld_qacc_h_l_128_ip(data, -32);
__builtin_riscv_esp_ld_qacc_l_h_128_ip(data, -64);
__builtin_riscv_esp_ld_qacc_l_l_128_ip(data, -80);
__builtin_riscv_esp_ld_ua_state_ip(data, 1504);
__builtin_riscv_esp_ldxq_32(data, 6, 1, 7, 1);
__builtin_riscv_esp_st_qacc_h_h_128_ip(data, -480);
__builtin_riscv_esp_st_qacc_h_l_128_ip(data, -1712);
__builtin_riscv_esp_st_qacc_l_h_128_ip(data, 960);
__builtin_riscv_esp_st_qacc_l_l_128_ip(data, 1920);
__builtin_riscv_esp_st_ua_state_ip(data, -1360);
__builtin_riscv_esp_stxq_32(data, 6, 2, 3, 0);
__builtin_riscv_esp_vld_128_ip(data, -1136, 0);
__builtin_riscv_esp_vld_128_xp(data, data, 5);
__builtin_riscv_esp_vld_h_64_ip(data, 1008, 4);
__builtin_riscv_esp_vld_h_64_xp(data, data, 2);
__builtin_riscv_esp_vld_l_64_ip(data, -304, 6);
__builtin_riscv_esp_vld_l_64_xp(data, data, 6);
__builtin_riscv_esp_vst_128_ip(0, data, -1216);
__builtin_riscv_esp_vst_128_xp(data, 6, data);
__builtin_riscv_esp_vst_h_64_ip(1, data, -456);
__builtin_riscv_esp_vst_h_64_xp(data, 2, data);
__builtin_riscv_esp_vst_l_64_ip(6, data, 664);
__builtin_riscv_esp_vst_l_64_xp(data, 4, data);
__builtin_riscv_esp_slci_2q(2, 0, 14);
__builtin_riscv_esp_slcxxp_2q(data, data, 0, 1);
__builtin_riscv_esp_src_q(7, 3, 2);
__builtin_riscv_esp_src_q_ld_ip(1, data, 4, 1168, 4);
__builtin_riscv_esp_src_q_ld_xp(data, 0, data, 1, 0);
__builtin_riscv_esp_src_q_qup(3, 3, 0);
__builtin_riscv_esp_srci_2q(7, 4, 1);
__builtin_riscv_esp_srcmb_s16_q_qacc(2, 1, 5);
__builtin_riscv_esp_srcmb_s16_qacc(data, 0, 7);
__builtin_riscv_esp_srcmb_s8_q_qacc(7, 0, 3);
__builtin_riscv_esp_srcmb_s8_qacc(data, 1, 3);
__builtin_riscv_esp_srcmb_u16_q_qacc(6, 1, 0);
__builtin_riscv_esp_srcmb_u16_qacc(data, 0, 0);
__builtin_riscv_esp_srcmb_u8_q_qacc(6, 0, 7);
__builtin_riscv_esp_srcmb_u8_qacc(data, 1, 2);
__builtin_riscv_esp_srcq_128_st_incp(0, 5, data);
__builtin_riscv_esp_srcxxp_2q(data, data, 7, 5);
__builtin_riscv_esp_srs_s_xacc(data, data);
__builtin_riscv_esp_srs_u_xacc(data, data);
__builtin_riscv_esp_vsl_32(0, 3);
__builtin_riscv_esp_vsld_16(6, 4, 4);
__builtin_riscv_esp_vsld_32(2, 7, 5);
__builtin_riscv_esp_vsld_8(1, 0, 0);
__builtin_riscv_esp_vsr_s32(6, 2);
__builtin_riscv_esp_vsr_u32(3, 2);
__builtin_riscv_esp_vsrd_16(6, 2, 1);
__builtin_riscv_esp_vsrd_32(7, 5, 4);
__builtin_riscv_esp_vsrd_8(2, 1, 4);
__builtin_riscv_esp_st_s_xacc_ip(data, 912);
__builtin_riscv_esp_st_u_xacc_ip(data, -112);
}
